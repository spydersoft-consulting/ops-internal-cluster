mimir-distributed:
  global:
    extraEnvFrom:
      - secretRef:
          name: mimir-minio-secret
  minio: 
    enabled: false
  alertmanager:
    enabled: true
    persistentVolume:
      storageClass: nfs-client
  ingester:
    persistentVolume:
      storageClass: nfs-client
  store_gateway:
    persistentVolume:
      storageClass: nfs-client
  compactor:
    persistentVolume:
      storageClass: nfs-client
  ruler:
    enabled: true
  nginx:
    ingress:
      enabled: true
      hosts:
        - host: "mimir.mattgerega.net"
          paths:
            - path: /
              pathType: Prefix
      annotations:
        kubernetes.io/ingress.class: nginx
        nginx.ingress.kubernetes.io/service-upstream: "true"
      tls: []
    nginxConfig:
      file: |
        worker_processes  5;  ## Default: 1
        error_log  /dev/stderr;
        pid        /tmp/nginx.pid;
        worker_rlimit_nofile 8192;
        events {
          worker_connections  4096;  ## Default: 1024
        }
        http {
          client_body_temp_path /tmp/client_temp;
          proxy_temp_path       /tmp/proxy_temp_path;
          fastcgi_temp_path     /tmp/fastcgi_temp;
          uwsgi_temp_path       /tmp/uwsgi_temp;
          scgi_temp_path        /tmp/scgi_temp;
          default_type application/octet-stream;
          log_format   {{ .Values.nginx.nginxConfig.logFormat }}
          {{- if .Values.nginx.verboseLogging }}
          access_log   /dev/stderr  main;
          {{- else }}
          map $status $loggable {
            ~^[23]  0;
            default 1;
          }
          access_log   /dev/stderr  main  if=$loggable;
          {{- end }}
          sendfile     on;
          tcp_nopush   on;
          {{- if .Values.nginx.nginxConfig.resolver }}
          resolver {{ .Values.nginx.nginxConfig.resolver }};
          {{- else }}
          resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};
          {{- end }}
          {{- with .Values.nginx.nginxConfig.httpSnippet }}
          {{ . | nindent 2 }}
          {{- end }}
          # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.
          map $http_x_scope_orgid $ensured_x_scope_orgid {
            default $http_x_scope_orgid;
            "" "{{ include "mimir.noAuthTenant" . }}";
          }
          server {
            listen 8080;
            {{- if .Values.nginx.basicAuth.enabled }}
            auth_basic           "Mimir";
            auth_basic_user_file /etc/nginx/secrets/.htpasswd;
            {{- end }}
            location = / {
              return 200 'OK';
              auth_basic off;
            }
            proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;
            # Distributor endpoints
            location /distributor {
              proxy_pass      http://{{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            location = /api/v1/push {
              proxy_pass      http://{{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            # Alertmanager endpoints
            location {{ template "mimir.alertmanagerHttpPrefix" . }} {
              proxy_pass      http://{{ template "mimir.fullname" . }}-alertmanager.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            location = /multitenant_alertmanager/status {
              proxy_pass      http://{{ template "mimir.fullname" . }}-alertmanager.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            location = /api/v1/alerts {
              proxy_pass      http://{{ template "mimir.fullname" . }}-alertmanager.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            # Ruler endpoints
            location {{- template "mimir.prometheusHttpPrefix" . }}/config/v1/rules {
              proxy_pass      http://{{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            location {{- template "mimir.prometheusHttpPrefix" . }}/api/v1/rules {
              proxy_pass      http://{{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            location {{- template "mimir.prometheusHttpPrefix" . }}/api/v1/alerts {
              proxy_pass      http://{{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            location = /ruler/ring {
              proxy_pass      http://{{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            # Rest of {{- template "mimir.prometheusHttpPrefix" . }} goes to the query frontend
            location {{- template "mimir.prometheusHttpPrefix" . }} {
              proxy_pass      http://{{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            # Buildinfo endpoint can go to any component
            location = /api/v1/status/buildinfo {
              proxy_pass      http://{{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            # Compactor endpoint for uploading blocks
            location /api/v1/upload/block/ {
              proxy_pass      http://{{ template "mimir.fullname" . }}-compactor.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            {{- with .Values.nginx.nginxConfig.serverSnippet }}
            {{ . | nindent 4 }}
            {{- end }}
          }
        }
  mimir:
    structuredConfig:
      api:
        prometheus_http_prefix: '/api/prom'
      alertmanager_storage:
        backend: s3
        s3:
          access_key_id: ${MINIO_USERNAME}
          bucket_name: mimir-ruler
          endpoint: 192.168.1.46:9000
          insecure: true
          secret_access_key: ${MINIO_PASSWORD}
      blocks_storage:
        s3:
          access_key_id: ${MINIO_USERNAME}
          bucket_name: mimir-tsdb
          endpoint: 192.168.1.46:9000
          insecure: true
          secret_access_key: ${MINIO_PASSWORD}
      ruler_storage:
        backend: s3
        s3:
          access_key_id: ${MINIO_USERNAME}
          bucket_name: mimir-ruler
          endpoint: 192.168.1.46:9000
          insecure: true
          secret_access_key: ${MINIO_PASSWORD}
      limits:
        max_global_series_per_user: 0
        ingestion_rate: 40000
        ingestion_burst_size: 100000
        out_of_order_time_window: 
  metaMonitoring:
    serviceMonitor:
      enabled: true
      interval: 1m
      scrapeTimeout: 30s
